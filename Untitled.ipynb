{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kolektif Öğrenme Yöntemleri \n",
    "## Final Projesi\n",
    "\n",
    "* Env: Python3\n",
    "* Author: **MOGHAM NJIKAM MOHAMED NOURDINE** || ÖĞRENCİ NUMARASI: **198229001004**\n",
    "* Function：Random Forest（RF)\n",
    "\n",
    "* DATA SOURCE: UCI. wine[DB/OL].https://archive.ics.uci.edu/ml/machine-learning-databases/wine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "\n",
    "**Import the usual libraries for Pandas, Numpy, sklearn, joblib and the other python mathematical libaries. You can import sklearn later on if you want.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas is used for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import collections\n",
    "from sklearn.datasets import load_digits\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree(object):\n",
    "    # Define a decision tree\n",
    "    def __init__(self):\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.leaf_value = None\n",
    "        self.tree_left = None\n",
    "        self.tree_right = None\n",
    "\n",
    "    def calc_predict_value(self, dataset):\n",
    "        # Find the leaf node to which the sample belongs through a recursive decision tree\n",
    "        if self.leaf_value is not None:\n",
    "            return self.leaf_value\n",
    "        elif dataset[self.split_feature] <= self.split_value:\n",
    "            return self.tree_left.calc_predict_value(dataset)\n",
    "        else:\n",
    "            return self.tree_right.calc_predict_value(dataset)\n",
    "\n",
    "    def describe_tree(self):\n",
    "        # Print decision tree in json format for easy viewing of tree structure\n",
    "        if not self.tree_left and not self.tree_right:\n",
    "            leaf_info = \"{leaf_value:\" + str(self.leaf_value) + \"}\"\n",
    "            return leaf_info\n",
    "        left_info = self.tree_left.describe_tree()\n",
    "        right_info = self.tree_right.describe_tree()\n",
    "        tree_structure = \"{'split_feature':\" + str(self.split_feature) + \\\n",
    "                         \",'split_value':\" + str(self.split_value) + \\\n",
    "                         \",'left_tree':\" + left_info + \\\n",
    "                         \",'right_tree':\" + right_info + \"}\"\n",
    "        return tree_structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "   ### Random forest parameters\n",
    "    ----------\n",
    "    n_estimators:      Number of trees\n",
    "    max_depth:         Tree depth, -1 means unlimited depth\n",
    "    min_samples_split: The minimum number of samples required for node splitting is less than this value\n",
    "    min_samples_leaf:  The minimum sample number of leaf nodes, less than this value leaves are merged\n",
    "    min_split_gain:    The minimum gain required for splitting is less than this value\n",
    "    colsample_bytree:  Column sampling settings can be selected [sqrt, log2]. sqrt means randomly select \n",
    "                       sqrt (n_features) features，log2 means randomly select log (n_features) features, \n",
    "                       set to other, no column sampling\n",
    "    subsample:         Line sampling ratio\n",
    "    random_state:      Random seeds, the n_estimators sample set generated each time after setting will \n",
    "                       not change, to ensure that the experiment can be repeated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RandomForestClassifier(object):\n",
    "    def __init__(self, n_estimators=10, max_depth=-1, min_samples_split=2, min_samples_leaf=1,\n",
    "                 min_split_gain=0.0, colsample_bytree=None, subsample=0.8, random_state=None):\n",
    "        \"\"\"\n",
    "        Random forest parameters\n",
    "        ----------\n",
    "        n_estimators:      Number of trees\n",
    "        max_depth:         Tree depth, -1 means unlimited depth\n",
    "        min_samples_split: The minimum number of samples required for node splitting is less than this value\n",
    "        min_samples_leaf:  The minimum sample number of leaf nodes, less than this value leaves are merged\n",
    "        min_split_gain:    The minimum gain required for splitting is less than this value\n",
    "        colsample_bytree:  Column sampling settings can be selected [sqrt, log2]. sqrt means randomly select \n",
    "                           sqrt (n_features) features，log2 means randomly select log (n_features) features, \n",
    "                           set to other, no column sampling\n",
    "        subsample:         Line sampling ratio\n",
    "        random_state:      Random seeds, the n_estimators sample set generated each time after setting will \n",
    "                           not change, to ensure that the experiment can be repeated\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth if max_depth != -1 else float('inf')\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_split_gain = min_split_gain\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.subsample = subsample\n",
    "        self.random_state = random_state\n",
    "        self.trees = None\n",
    "        self.feature_importances_ = dict()\n",
    "\n",
    "    def fit(self, dataset, targets):\n",
    "        \"\"\"Model training entrance\"\"\"\n",
    "        assert targets.unique().__len__() == 2, \"There must be two class for targets!\"\n",
    "        targets = targets.to_frame(name='label')\n",
    "\n",
    "        if self.random_state:\n",
    "            random.seed(self.random_state)\n",
    "        random_state_stages = random.sample(range(self.n_estimators), self.n_estimators)\n",
    "\n",
    "        # Two column sampling methods\n",
    "        if self.colsample_bytree == \"sqrt\":\n",
    "            self.colsample_bytree = int(len(dataset.columns) ** 0.5)\n",
    "        elif self.colsample_bytree == \"log2\":\n",
    "            self.colsample_bytree = int(math.log(len(dataset.columns)))\n",
    "        else:\n",
    "            self.colsample_bytree = len(dataset.columns)\n",
    "\n",
    "        # Building multiple decision trees in parallel\n",
    "        self.trees = Parallel(n_jobs=-1, verbose=0, backend=\"threading\")(\n",
    "            delayed(self._parallel_build_trees)(dataset, targets, random_state)\n",
    "                for random_state in random_state_stages)\n",
    "        \n",
    "    def _parallel_build_trees(self, dataset, targets, random_state):\n",
    "        \"\"\"Bootstrap has put back sampling to generate training sample set and build decision tree\"\"\"\n",
    "        subcol_index = random.sample(dataset.columns.tolist(), self.colsample_bytree)\n",
    "        dataset_stage = dataset.sample(n=int(self.subsample * len(dataset)), replace=True, \n",
    "                                        random_state=random_state).reset_index(drop=True)\n",
    "        dataset_stage = dataset_stage.loc[:, subcol_index]\n",
    "        targets_stage = targets.sample(n=int(self.subsample * len(dataset)), replace=True, \n",
    "                                        random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "        tree = self._build_single_tree(dataset_stage, targets_stage, depth=0)\n",
    "        print(tree.describe_tree())\n",
    "        return tree\n",
    "\n",
    "    def _build_single_tree(self, dataset, targets, depth):\n",
    "        \"\"\"Recursively build a decision tree\"\"\"\n",
    "        # If the categories of the node are all the same / samples are smaller than the minimum \n",
    "        # number of samples required for splitting, the category with the most occurrences is selected. \n",
    "        # Stop splitting\n",
    "        if len(targets['label'].unique()) <= 1 or dataset.__len__() <= self.min_samples_split:\n",
    "            tree = Tree()\n",
    "            tree.leaf_value = self.calc_leaf_value(targets['label'])\n",
    "            return tree\n",
    "\n",
    "        if depth < self.max_depth:\n",
    "            best_split_feature, best_split_value, best_split_gain = self.choose_best_feature(dataset, targets)\n",
    "            left_dataset, right_dataset, left_targets, right_targets = \\\n",
    "                self.split_dataset(dataset, targets, best_split_feature, best_split_value)\n",
    "\n",
    "            tree = Tree()\n",
    "            # If the sample of the left leaf node / right leaf node is smaller than the set minimum \n",
    "            # sample number of leaf nodes after the split\n",
    "            if left_dataset.__len__() <= self.min_samples_leaf or \\\n",
    "                    right_dataset.__len__() <= self.min_samples_leaf or \\\n",
    "                    best_split_gain <= self.min_split_gain:\n",
    "                tree.leaf_value = self.calc_leaf_value(targets['label'])\n",
    "                return tree\n",
    "            else:\n",
    "                # If the feature is used during splitting, the import of the feature is increased by 1.\n",
    "                self.feature_importances_[best_split_feature] = \\\n",
    "                    self.feature_importances_.get(best_split_feature, 0) + 1\n",
    "\n",
    "                tree.split_feature = best_split_feature\n",
    "                tree.split_value = best_split_value\n",
    "                tree.tree_left = self._build_single_tree(left_dataset, left_targets, depth+1)\n",
    "                tree.tree_right = self._build_single_tree(right_dataset, right_targets, depth+1)\n",
    "                return tree\n",
    "        # If the depth of the tree exceeds the preset value, the split is terminated\n",
    "        else:\n",
    "            tree = Tree()\n",
    "            tree.leaf_value = self.calc_leaf_value(targets['label'])\n",
    "            return tree\n",
    "\n",
    "    def choose_best_feature(self, dataset, targets):\n",
    "        \"\"\"Find the best way to divide the data set，Find the optimal split feature,Split threshold,Split gain\"\"\"\n",
    "        best_split_gain = 1\n",
    "        best_split_feature = None\n",
    "        best_split_value = None\n",
    "\n",
    "        for feature in dataset.columns:\n",
    "            if dataset[feature].unique().__len__() <= 100:\n",
    "                unique_values = sorted(dataset[feature].unique().tolist())\n",
    "            # If the dimension feature has too many values, select 100 percentile values \n",
    "            # as the candidate split threshold\n",
    "            else:\n",
    "                unique_values = np.unique([np.percentile(dataset[feature], x)\n",
    "                                           for x in np.linspace(0, 100, 100)])\n",
    "\n",
    "            # Find the splitting gain for the possible splitting threshold and \n",
    "            # select the threshold with the largest gain\n",
    "            for split_value in unique_values:\n",
    "                left_targets = targets[dataset[feature] <= split_value]\n",
    "                right_targets = targets[dataset[feature] > split_value]\n",
    "                split_gain = self.calc_gini(left_targets['label'], right_targets['label'])\n",
    "\n",
    "                if split_gain < best_split_gain:\n",
    "                    best_split_feature = feature\n",
    "                    best_split_value = split_value\n",
    "                    best_split_gain = split_gain\n",
    "        return best_split_feature, best_split_value, best_split_gain\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_leaf_value(targets):\n",
    "        \"\"\"Select the category with the most occurrences in the sample as the value of the leaf node\"\"\"\n",
    "        label_counts = collections.Counter(targets)\n",
    "        major_label = max(zip(label_counts.values(), label_counts.keys()))\n",
    "        return major_label[1]\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_gini(left_targets, right_targets):\n",
    "        \"\"\"The classification tree uses the Gini index as an indicator to select the optimal split point\"\"\"\n",
    "        split_gain = 0\n",
    "        for targets in [left_targets, right_targets]:\n",
    "            gini = 1\n",
    "            # Count how many samples are in each category, and then calculate gini\n",
    "            label_counts = collections.Counter(targets)\n",
    "            for key in label_counts:\n",
    "                prob = label_counts[key] * 1.0 / len(targets)\n",
    "                gini -= prob ** 2\n",
    "            split_gain += len(targets) * 1.0 / (len(left_targets) + len(right_targets)) * gini\n",
    "        return split_gain\n",
    "\n",
    "    @staticmethod\n",
    "    def split_dataset(dataset, targets, split_feature, split_value):\n",
    "        \"\"\"Divide the sample into left and right according to characteristics and threshold, \n",
    "         the left is less than or equal to the threshold, the right is greater than the threshold\"\"\"\n",
    "        left_dataset = dataset[dataset[split_feature] <= split_value]\n",
    "        left_targets = targets[dataset[split_feature] <= split_value]\n",
    "        right_dataset = dataset[dataset[split_feature] > split_value]\n",
    "        right_targets = targets[dataset[split_feature] > split_value]\n",
    "        return left_dataset, right_dataset, left_targets, right_targets\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        \"\"\"Enter a sample to predict the category\"\"\"\n",
    "        res = []\n",
    "        for _, row in dataset.iterrows():\n",
    "            pred_list = []\n",
    "            # Count the prediction results of each tree, and select the result with \n",
    "            # the most occurrences as the final category\n",
    "            for tree in self.trees:\n",
    "                pred_list.append(tree.calc_predict_value(row))\n",
    "\n",
    "            pred_label_counts = collections.Counter(pred_list)\n",
    "            pred_label = max(zip(pred_label_counts.values(), pred_label_counts.keys()))\n",
    "            res.append(pred_label[1])\n",
    "        return np.array(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  Total phenols  \\\n",
       "0    14.23        1.71  2.43               15.6        127           2.80   \n",
       "1    13.20        1.78  2.14               11.2        100           2.65   \n",
       "2    13.16        2.36  2.67               18.6        101           2.80   \n",
       "3    14.37        1.95  2.50               16.8        113           3.85   \n",
       "4    13.24        2.59  2.87               21.0        118           2.80   \n",
       "\n",
       "   Flavanoids  Nonflavanoid phenols  Proanthocyanins  Color intensity   Hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   OD280/OD315 of diluted wines  Proline  label  \n",
       "0                          3.92     1065      1  \n",
       "1                          3.40     1050      1  \n",
       "2                          3.17     1185      1  \n",
       "3                          3.45     1480      1  \n",
       "4                          2.93      735      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data and display first 5 rows\n",
    "df = pd.read_csv(\"source/wine.txt\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "#check labels\n",
    "print(df.label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our features is: (178, 14)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of our features is:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.Relevant Information:\n",
    "\n",
    "   The attributes are:\n",
    "   \n",
    " \t1) Alcohol\n",
    " \t2) Malic acid\n",
    " \t3) Ash\n",
    "\t 4) Alcalinity of ash  \n",
    " \t5) Magnesium\n",
    "\t 6) Total phenols\n",
    " \t7) Flavanoids\n",
    " \t8) Nonflavanoid phenols\n",
    " \t9) Proanthocyanins\n",
    "\t 10)Color intensity\n",
    " \t11)Hue\n",
    " \t12)OD280/OD315 of diluted wines\n",
    " \t13)Proline            \n",
    "\n",
    "#### 2. Number of Instances\n",
    "\n",
    "    class 1 59\n",
    "\tclass 2 71\n",
    "\tclass 3 48\n",
    "\n",
    "#### 3. Number of Attributes \n",
    "\t\n",
    "\t13\n",
    "\n",
    "##### 4. For Each Attribute:\n",
    "\n",
    "\tAll attributes are continuous\n",
    "\t\n",
    "\tNo statistics available, but suggest to standardise\n",
    "\tvariables for certain uses (e.g. for us with classifiers\n",
    "\twhich are NOT scale invariant)\n",
    "\n",
    "\tNOTE: 1st attribute is class identifier (1-3)\n",
    "\n",
    "##### 5. Missing Attribute Values:\n",
    "\n",
    "\tNone\n",
    "\n",
    "##### 6. Class Distribution: number of instances per class\n",
    "\n",
    "    class 1 59\n",
    "\tclass 2 71\n",
    "\tclass 3 48\n",
    "    \n",
    " #### Details information can be found in the official website\n",
    "     \n",
    "     DATA SOURCE: UCI.wine[DB/OL].\n",
    "     \n",
    "     https://archive.ics.uci.edu/ml/machine-learning-databases/wine\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.000618</td>\n",
       "      <td>2.336348</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>99.741573</td>\n",
       "      <td>2.295112</td>\n",
       "      <td>2.029270</td>\n",
       "      <td>0.361854</td>\n",
       "      <td>1.590899</td>\n",
       "      <td>5.058090</td>\n",
       "      <td>0.957449</td>\n",
       "      <td>2.611685</td>\n",
       "      <td>746.893258</td>\n",
       "      <td>1.938202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.811827</td>\n",
       "      <td>1.117146</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>14.282484</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>0.124453</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>2.318286</td>\n",
       "      <td>0.228572</td>\n",
       "      <td>0.709990</td>\n",
       "      <td>314.907474</td>\n",
       "      <td>0.775035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.030000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.362500</td>\n",
       "      <td>1.602500</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>1.742500</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>500.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.050000</td>\n",
       "      <td>1.865000</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>2.355000</td>\n",
       "      <td>2.135000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.555000</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>673.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.677500</td>\n",
       "      <td>3.082500</td>\n",
       "      <td>2.557500</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.830000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1680.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Alcohol  Malic acid         Ash  Alcalinity of ash   Magnesium  \\\n",
       "count  178.000000  178.000000  178.000000         178.000000  178.000000   \n",
       "mean    13.000618    2.336348    2.366517          19.494944   99.741573   \n",
       "std      0.811827    1.117146    0.274344           3.339564   14.282484   \n",
       "min     11.030000    0.740000    1.360000          10.600000   70.000000   \n",
       "25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n",
       "50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n",
       "75%     13.677500    3.082500    2.557500          21.500000  107.000000   \n",
       "max     14.830000    5.800000    3.230000          30.000000  162.000000   \n",
       "\n",
       "       Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "count     178.000000  178.000000            178.000000       178.000000   \n",
       "mean        2.295112    2.029270              0.361854         1.590899   \n",
       "std         0.625851    0.998859              0.124453         0.572359   \n",
       "min         0.980000    0.340000              0.130000         0.410000   \n",
       "25%         1.742500    1.205000              0.270000         1.250000   \n",
       "50%         2.355000    2.135000              0.340000         1.555000   \n",
       "75%         2.800000    2.875000              0.437500         1.950000   \n",
       "max         3.880000    5.080000              0.660000         3.580000   \n",
       "\n",
       "       Color intensity         Hue  OD280/OD315 of diluted wines      Proline  \\\n",
       "count       178.000000  178.000000                    178.000000   178.000000   \n",
       "mean          5.058090    0.957449                      2.611685   746.893258   \n",
       "std           2.318286    0.228572                      0.709990   314.907474   \n",
       "min           1.280000    0.480000                      1.270000   278.000000   \n",
       "25%           3.220000    0.782500                      1.937500   500.500000   \n",
       "50%           4.690000    0.965000                      2.780000   673.500000   \n",
       "75%           6.200000    1.120000                      3.170000   985.000000   \n",
       "max          13.000000    1.710000                      4.000000  1680.000000   \n",
       "\n",
       "            label  \n",
       "count  178.000000  \n",
       "mean     1.938202  \n",
       "std      0.775035  \n",
       "min      1.000000  \n",
       "25%      1.000000  \n",
       "50%      2.000000  \n",
       "75%      3.000000  \n",
       "max      3.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.hist(df.label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"source/wine.txt\")\n",
    "# labels = np.array(df['label'])\n",
    "# df = df.drop('Proline', axis = 1)\n",
    "# df_list = list(df.columns)\n",
    "# df = np.array(df)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_features, test_features, train_labels, test_labels = train_test_split(df, labels, test_size = .35, random_state = 66)\n",
    "\n",
    "# print('Training Features Shape:', train_features.shape)\n",
    "# print('Training Labels Shape:', train_labels.shape)\n",
    "# print('Testing Features Shape', test_features.shape)\n",
    "# print('Testing Labels Shape', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are not any data points that immediately appear as anomalous and no zeros in any of the measurement columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'split_feature':Malic acid,'split_value':1.41,'left_tree':{leaf_value:2},'right_tree':{'split_feature':OD280/OD315 of diluted wines,'split_value':2.65,'left_tree':{'split_feature':OD280/OD315 of diluted wines,'split_value':2.5,'left_tree':{leaf_value:2},'right_tree':{leaf_value:2}},'right_tree':{'split_feature':Ash,'split_value':2.3,'left_tree':{'split_feature':OD280/OD315 of diluted wines,'split_value':3.14,'left_tree':{leaf_value:2},'right_tree':{leaf_value:1}},'right_tree':{leaf_value:1}}}}\n",
      "{'split_feature':Proline,'split_value':985,'left_tree':{'split_feature':Magnesium,'split_value':110,'left_tree':{leaf_value:2},'right_tree':{leaf_value:1}},'right_tree':{leaf_value:1}}\n",
      "{'split_feature':Malic acid,'split_value':1.51,'left_tree':{'split_feature':Malic acid,'split_value':1.47,'left_tree':{leaf_value:2},'right_tree':{leaf_value:2}},'right_tree':{'split_feature':OD280/OD315 of diluted wines,'split_value':2.5,'left_tree':{leaf_value:2},'right_tree':{'split_feature':Malic acid,'split_value':2.02,'left_tree':{'split_feature':OD280/OD315 of diluted wines,'split_value':3.56,'left_tree':{leaf_value:1},'right_tree':{leaf_value:1}},'right_tree':{'split_feature':Nonflavanoid phenols,'split_value':0.22,'left_tree':{leaf_value:1},'right_tree':{'split_feature':Malic acid,'split_value':2.08,'left_tree':{leaf_value:2},'right_tree':{leaf_value:2}}}}}}\n",
      "{'split_feature':Ash,'split_value':2.31,'left_tree':{'split_feature':Nonflavanoid phenols,'split_value':0.22,'left_tree':{'split_feature':Proanthocyanins,'split_value':2.5,'left_tree':{leaf_value:2},'right_tree':{leaf_value:1}},'right_tree':{'split_feature':Ash,'split_value':2.13,'left_tree':{leaf_value:2},'right_tree':{'split_feature':Proanthocyanins,'split_value':1.9,'left_tree':{'split_feature':Proanthocyanins,'split_value':1.63,'left_tree':{leaf_value:2},'right_tree':{leaf_value:2}},'right_tree':{leaf_value:1}}}},'right_tree':{'split_feature':Proanthocyanins,'split_value':1.35,'left_tree':{leaf_value:2},'right_tree':{leaf_value:1}}}\n",
      "{'split_feature':Total phenols,'split_value':2.56,'left_tree':{'split_feature':Total phenols,'split_value':2.1,'left_tree':{leaf_value:2},'right_tree':{leaf_value:2}},'right_tree':{'split_feature':Ash,'split_value':2.16,'left_tree':{leaf_value:2},'right_tree':{'split_feature':Ash,'split_value':2.61,'left_tree':{'split_feature':Total phenols,'split_value':2.88,'left_tree':{leaf_value:1},'right_tree':{leaf_value:1}},'right_tree':{leaf_value:2}}}}\n",
      "Mean Absolute Error: 0.08 degrees.\n",
      "0.9239130434782609\n",
      "0.8205128205128205\n"
     ]
    }
   ],
   "source": [
    "df = df[df['label'].isin([1, 2])].sample(frac=1, random_state=66).reset_index(drop=True)\n",
    "clf = RandomForestClassifier(n_estimators=5,\n",
    "                             max_depth=5,\n",
    "                             min_samples_split=6,\n",
    "                             min_samples_leaf=2,\n",
    "                             min_split_gain=0.0,\n",
    "                             colsample_bytree=\"sqrt\",\n",
    "                             subsample=0.8,\n",
    "                             random_state=66)\n",
    "train_count = int(0.7 * len(df))\n",
    "feature_list = [\"Alcohol\", \"Malic acid\", \"Ash\", \"Alcalinity of ash\", \"Magnesium\", \"Total phenols\", \n",
    "                \"Flavanoids\", \"Nonflavanoid phenols\", \"Proanthocyanins\", \"Color intensity\", \"Hue\", \n",
    "                \"OD280/OD315 of diluted wines\", \"Proline\"]\n",
    "\n",
    "train_features = df.loc[:train_count, feature_list]\n",
    "train_labels = df.loc[:train_count, 'label']\n",
    "\n",
    "clf.fit(train_features , train_labels)\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = clf.predict(train_features)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - train_labels)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(df.loc[:train_count, 'label'], clf.predict(df.loc[:train_count, feature_list])))\n",
    "print(metrics.accuracy_score(df.loc[train_count:, 'label'], clf.predict(df.loc[train_count:, feature_list])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
